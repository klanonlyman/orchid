{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ABfpq94t_0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import data_augment_tool\n",
        "import csv\n",
        "df=pd.read_csv(\"data//label.csv\")\n",
        "filename=df[\"filename\"]\n",
        "category=df[\"category\"]\n",
        "if os.path.exists(\"./data_augmentation\") is False:\n",
        "      os.makedirs(\"./data_augmentation\")\n",
        "with open(\"data_augmentation//label.csv\",\"a+\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"filename\",\"category\"])\n",
        "    for i in range(0,len(filename)):\n",
        "        label=category[i]\n",
        "        name=filename[i]\n",
        "        img_path=os.path.join(\"data//\",name)\n",
        "        img=cv2.imread(img_path)\n",
        "        \n",
        " \n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",name),img)\n",
        "        writer.writerow([name,label])\n",
        "        \n",
        "        after=data_augment_tool.colorjitter(img,\"b\")\n",
        "        after_name=\"b_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.colorjitter(img,\"s\")\n",
        "        after_name=\"s_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.colorjitter(img,\"c\")\n",
        "        after_name=\"c_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.noisy(img,\"gauss\")\n",
        "        after_name=\"gauss_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.noisy(img,\"sp\")\n",
        "        after_name=\"sp_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.filters(img,\"blur\")\n",
        "        after_name=\"blur_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.filters(img,\"gaussian\")\n",
        "        after_name=\"gaussian_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "\n",
        "        after=data_augment_tool.filters(img,\"median\")\n",
        "        after_name=\"median_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "        \n",
        "        after=data_augment_tool.h_mirror(img)\n",
        "        after_name=\"mirrorh_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "        \n",
        "        \n",
        "        after=data_augment_tool.v_mirror(img)\n",
        "        after_name=\"mirrorv_\"+name\n",
        "        cv2.imwrite(os.path.join(\"data_augmentation//\",after_name),after)\n",
        "        writer.writerow([after_name,label])\n",
        "        \n",
        "        \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import data_augment_tool\n",
        "import csv\n",
        "df=pd.read_csv(\"data_augmentation//label.csv\")\n",
        "if os.path.exists(\"./train\") is False:\n",
        "      os.makedirs(\"./train\")\n",
        "for i in range(0,len(df)):\n",
        "    sub=df.loc[i]\n",
        "    filename=sub[\"filename\"]\n",
        "    category=str(sub[\"category\"])\n",
        "    floder=os.path.join(\"train\",category)\n",
        "    if not(os.path.exists(floder)):\n",
        "        os.mkdir(floder)\n",
        "    img=cv2.imread(os.path.join(\"data_augmentation\",filename))\n",
        "    cv2.imwrite(os.path.join(floder,filename),img)\n"
      ],
      "metadata": {
        "id": "JYcsq8ae5l0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "\n",
        "from my_dataset import MyDataSet\n",
        "from model import swin_large_patch4_window12_384_in22k as create_model\n",
        "from utils import read_split_data,train_one_epoch, evaluate\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  data_path=\"train\"\n",
        "  weight_path=\"./swin_large_patch4_window12_384_22k.pth\"\n",
        "  device=\"cuda:0\"\n",
        "  num_classes=219\n",
        "  epochs=100\n",
        "  batch_size=2\n",
        "  lr=2e-6\n",
        "  \n",
        "  \n",
        "  device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "  if os.path.exists(\"./weights\") is False:\n",
        "      os.makedirs(\"./weights\")\n",
        "      \n",
        "  tb_writer = SummaryWriter()\n",
        "  train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path)\n",
        "  img_size = 384\n",
        "  data_transform = {\n",
        "      \"train\": transforms.Compose([transforms.RandomResizedCrop(img_size),\n",
        "                                   transforms.RandomHorizontalFlip(),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "      }\n",
        "  train_dataset = MyDataSet(images_path=train_images_path,\n",
        "                                images_class=train_images_label,\n",
        "                                transform=data_transform[\"train\"])\n",
        "  nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 shuffle=True,\n",
        "                                                 pin_memory=True,\n",
        "                                                 num_workers=nw,\n",
        "                                                 collate_fn=train_dataset.collate_fn)\n",
        "  \n",
        "  model = create_model(num_classes=num_classes).to(device)\n",
        "  \n",
        "  \n",
        "  if weight_path != \"\":\n",
        "      assert os.path.exists(weight_path), \"weights file: '{}' not exist.\".format(weight_path)\n",
        "      weights_dict = torch.load(weight_path, map_location=device)[\"model\"]\n",
        "      # 删除有关分类类别的权重\n",
        "      for k in list(weights_dict.keys()):\n",
        "          if \"head\" in k:\n",
        "              del weights_dict[k]\n",
        "      print(model.load_state_dict(weights_dict, strict=False))\n",
        "      \n",
        "  pg = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = optim.AdamW(pg, lr=lr, weight_decay=1e-7)\n",
        "  for epoch in range(epochs):\n",
        "      # train\n",
        "      train_loss, train_acc = train_one_epoch(model=model,\n",
        "                                              optimizer=optimizer,\n",
        "                                              data_loader=train_loader,\n",
        "                                              device=device,\n",
        "                                              epoch=epoch)\n",
        "  \n",
        "  \n",
        "  \n",
        "      torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqwor-pzEOvV",
        "outputId": "e399d10e-0da7-44bc-e342-4cea44537a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from model import swin_large_patch4_window12_384_in22k as create_model\n",
        "import numpy as np\n",
        "import csv\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x\n",
        "def test():\n",
        "    test_path = \"./test\"\n",
        "    json_path = './class_indices.json'\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_classes = 219\n",
        "    img_size = 384\n",
        "    data_transform = transforms.Compose([transforms.Resize([img_size,img_size]),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        "        )\n",
        "\n",
        "    json_file = open(json_path, \"r\")\n",
        "    class_indict = json.load(json_file)\n",
        "    class_list=list(class_indict.keys())\n",
        "    class_list.append(\"img\")\n",
        "    model = create_model(num_classes=num_classes).to(device)\n",
        "    list_= [\"55\",\"57\",\"62\"]\n",
        "    for item in list_:\n",
        "        print(\"model: %s\\n\"%item)\n",
        "        model_name = 'model-%s'%item\n",
        "        model_weight_path = \"./weights/%s.pth\"%model_name\n",
        "        model.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
        "        model.eval()\n",
        "        model.cuda()\n",
        "        file = os.listdir(test_path)\n",
        "        number=0\n",
        "        with open(\"result//\"+item+'.csv', 'a+', newline='') as student_file:\n",
        "            writer = csv.writer(student_file)\n",
        "            writer.writerow(class_list)\n",
        "            for img_name in file:\n",
        "                if number%1000==0:\n",
        "                    print(number)\n",
        "                img_path=os.path.join(test_path,img_name)\n",
        "                img = Image.open(img_path)\n",
        "                img = data_transform(img)\n",
        "                img = torch.unsqueeze(img, dim=0)\n",
        "                with torch.no_grad():\n",
        "                    output = torch.squeeze(model(img.to(device))).cpu()\n",
        "                ensemble_output = output.numpy()\n",
        "                ensemble_output=list(ensemble_output)\n",
        "                ensemble_output.append(img_name)\n",
        "                writer.writerow(ensemble_output)\n",
        "                number+=1\n",
        "if __name__ == '__main__':\n",
        "    if os.path.exists(\"./result\") is False:\n",
        "        os.makedirs(\"./result\")\n",
        "    test()\n",
        "    \n",
        "    json_path = './class_indices.json'\n",
        "    json_file = open(json_path, \"r\")\n",
        "    class_indict = json.load(json_file)\n",
        "    class_list=list(class_indict.keys())\n",
        "    df_55=pd.read_csv(\"result//55.csv\")\n",
        "    df_57=pd.read_csv(\"result//57.csv\")\n",
        "    df_62=pd.read_csv(\"result//62.csv\")\n",
        "    \n",
        "    with open(\"result//55_57_62_Sub.csv\", 'a+', newline='') as student_file:\n",
        "        writer = csv.writer(student_file)\n",
        "        writer.writerow([\"filename\",\"category\"])\n",
        "        for i in range(0,len(df_57)):\n",
        "            sub_55=df_55.loc[i][class_list]\n",
        "            sub_55=np.array(sub_55).astype(\"float32\")\n",
        "            sub_57=df_57.loc[i][class_list]\n",
        "            sub_57=np.array(sub_57).astype(\"float32\")\n",
        "            sub_62=df_62.loc[i][class_list]\n",
        "            sub_62=np.array(sub_62).astype(\"float32\")\n",
        "\n",
        "            \n",
        "            sub=(sub_62+sub_57+sub_55)/3\n",
        "            sub=torch.from_numpy(sub).cpu()\n",
        "            predict = torch.softmax(sub, dim=0)\n",
        "            predict_cla = torch.argmax(predict).numpy()\n",
        "            predict=class_indict[str(predict_cla)]\n",
        "            writer.writerow([df_57.loc[i][\"img\"],predict])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j1p3_qLXFgyr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "0d62a73a-6c8a-4dc5-90af-48a4a529fce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24089 images were found in the dataset.\n",
            "24089 images for training.\n",
            "0 images for validation.\n",
            "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask'])\n",
            "[train epoch 0] loss: 2.736, acc: 0.534: 100%|██████████| 12045/12045 [1:50:28<00:00,  1.82it/s]\n",
            "[train epoch 1] loss: 0.565, acc: 0.916: 100%|██████████| 12045/12045 [1:50:09<00:00,  1.82it/s]\n",
            "[train epoch 2] loss: 0.349, acc: 0.949:  19%|█▊        | 2233/12045 [20:25<1:29:46,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-25ae2240eb03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                                               \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                               \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                               epoch=epoch)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0msample_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0maccu_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_Ans1XU9E0yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7qisw5zZGL3_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}